{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LTalpa/ltp-critical-questions/blob/main/evaluation/LANGTECH_binary_prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywxlWqhtyMLg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import csv\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# =============================================================================\n",
        "# DATA LOADING\n",
        "# =============================================================================\n",
        "\n",
        "def load_data(csv_file_path):\n",
        "    \"\"\"Load data from CSV file into list of dicts.\"\"\"\n",
        "    with open(csv_file_path, 'r', encoding='utf-8') as f:\n",
        "        return list(csv.DictReader(f))\n",
        "\n",
        "def load_prompt_template(prompt_file):\n",
        "    \"\"\"Load prompt template from file.\"\"\"\n",
        "    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
        "        return f.read().strip()\n",
        "\n",
        "# =============================================================================\n",
        "# MODEL MANAGEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def load_model(model_name):\n",
        "    \"\"\"Load model and tokenizer.\"\"\"\n",
        "    print(f\"Loading {model_name}...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
        "        )\n",
        "\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        print(f\"✓ Loaded {model_name}\")\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        print(f\"✗ Failed to load {model_name}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def cleanup_model():\n",
        "    \"\"\"Clear GPU memory.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# =============================================================================\n",
        "# PREDICTION\n",
        "# =============================================================================\n",
        "\n",
        "def create_prompt(intervention, cq, template):\n",
        "    \"\"\"Create prompt from template.\"\"\"\n",
        "    return template.format(intervention=intervention, cq=cq)\n",
        "\n",
        "def generate_prediction(prompt, tokenizer, model):\n",
        "    \"\"\"Generate model prediction.\"\"\"\n",
        "    # Format with chat template\n",
        "    if tokenizer.chat_template:\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        formatted_input = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "    else:\n",
        "        formatted_input = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Generate\n",
        "    inputs = tokenizer(formatted_input, return_tensors=\"pt\")\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.to(\"cuda\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs.input_ids,\n",
        "            max_new_tokens=100,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id\n",
        "        )\n",
        "\n",
        "    # Decode response\n",
        "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    response = re.sub(r'<\\|assistant\\|>|</think>|header|system|user|assistant|subject', \"\", response)\n",
        "    response = response.strip()\n",
        "\n",
        "    return classify_response(response)\n",
        "\n",
        "def classify_response(response):\n",
        "    \"\"\"Classify response into Useful/Unhelpful/Invalid.\"\"\"\n",
        "    response_lower = response.lower()\n",
        "\n",
        "    if 'good' in response_lower:\n",
        "        return 'good'\n",
        "    elif 'bad' in response_lower:\n",
        "        return 'bad'\n",
        "    elif 'invalid' in response_lower:\n",
        "        return 'bad'\n",
        "    else:\n",
        "        return response  # Return raw response if no clear classification\n",
        "\n",
        "# =============================================================================\n",
        "# EVALUATION\n",
        "# =============================================================================\n",
        "\n",
        "def evaluate_single_item(item, tokenizer, model, template):\n",
        "    \"\"\"Evaluate single CQ item.\"\"\"\n",
        "    prompt = create_prompt(item['intervention'], item['cq'], template)\n",
        "    prediction = generate_prediction(prompt, tokenizer, model)\n",
        "\n",
        "    return {\n",
        "        'intervention_id': item['intervention_id'],\n",
        "        'cq_id': item['cq_id'],\n",
        "        'intervention': item['intervention'],\n",
        "        'cq': item['cq'],\n",
        "        'ground_truth': item['label'],\n",
        "        'prediction': prediction\n",
        "    }\n",
        "\n",
        "def run_evaluation(data_file, prompt_file, output_file='results.csv'):\n",
        "    \"\"\"Run complete evaluation.\"\"\"\n",
        "    # Load data and template\n",
        "    data = load_data(data_file)\n",
        "    template = load_prompt_template(prompt_file)\n",
        "\n",
        "    models = [\n",
        "        \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "        \"Qwen/Qwen3-1.7B\",\n",
        "        \"tiiuae/Falcon3-1B-Instruct\"\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    for model_name in models:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Evaluating with: {model_name}\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        # Load model\n",
        "        tokenizer, model = load_model(model_name)\n",
        "        if tokenizer is None or model is None:\n",
        "            continue\n",
        "\n",
        "        # Evaluate each item\n",
        "        for i, item in enumerate(data):\n",
        "            print(f\"Processing {i+1}/{len(data)}: {item['cq_id']}\")\n",
        "\n",
        "            result = evaluate_single_item(item, tokenizer, model, template)\n",
        "            result['model'] = model_name\n",
        "            all_results.append(result)\n",
        "\n",
        "            # Save progress every 10 items\n",
        "            if (i + 1) % 10 == 0:\n",
        "                save_results(all_results, output_file)\n",
        "                print(f\"Progress saved at {i+1} items\")\n",
        "\n",
        "        # Cleanup\n",
        "        del tokenizer, model\n",
        "        cleanup_model()\n",
        "        print(f\"Completed {model_name}\")\n",
        "\n",
        "    # Final save\n",
        "    save_results(all_results, output_file)\n",
        "    print(f\"\\nEvaluation complete. Results saved to {output_file}\")\n",
        "    return all_results\n",
        "\n",
        "# =============================================================================\n",
        "# RESULTS MANAGEMENT\n",
        "# =============================================================================\n",
        "\n",
        "def save_results(results, filename):\n",
        "    \"\"\"Save results to CSV.\"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(filename, index=False)\n",
        "\n",
        "def calculate_accuracy(results_file='results.csv'):\n",
        "    \"\"\"Calculate accuracy for each model.\"\"\"\n",
        "    df = pd.read_csv(results_file)\n",
        "\n",
        "    print(f\"\\nAccuracy Results:\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for model in df['model'].unique():\n",
        "        model_data = df[df['model'] == model]\n",
        "        accuracy = accuracy_score(model_data['ground_truth'], model_data['prediction'])\n",
        "        print(f\"{model}: {accuracy:.4f}\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mAVrqMLp8IY"
      },
      "outputs": [],
      "source": [
        "results = run_evaluation('/content/extracted_intervention_cq_pairs.csv', \"/content/binary_prompt.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def reformat_results(input_file, output_file='results_wide.csv'):\n",
        "    \"\"\"Convert long format to wide format with model predictions as columns.\"\"\"\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Pivot: keep base columns, spread model predictions across columns\n",
        "    wide_df = df.pivot_table(\n",
        "        index=['intervention_id', 'cq_id', 'intervention', 'cq', 'ground_truth'],\n",
        "        columns='model',\n",
        "        values='prediction',\n",
        "        aggfunc='first'\n",
        "    ).reset_index()\n",
        "\n",
        "    # Flatten column names\n",
        "    wide_df.columns.name = None\n",
        "\n",
        "    wide_df.to_csv(output_file, index=False)\n",
        "    print(f\"Reformatted results saved to {output_file}\")\n",
        "    return wide_df\n",
        "\n"
      ],
      "metadata": {
        "id": "QO0I83YlSw-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "def add_majority_vote(input_file, output_file='results_with_majority.csv'):\n",
        "    \"\"\"Add majority vote column to wide format CSV file.\"\"\"\n",
        "    wide_df = pd.read_csv(input_file)\n",
        "\n",
        "    # Get model columns (exclude base info columns)\n",
        "    base_cols = ['intervention_id', 'cq_id', 'intervention', 'cq', 'ground_truth']\n",
        "    model_cols = [col for col in wide_df.columns if col not in base_cols]\n",
        "\n",
        "    # Calculate majority vote for each row\n",
        "    majority_votes = []\n",
        "    for _, row in wide_df.iterrows():\n",
        "        predictions = [row[col] for col in model_cols if pd.notna(row[col])]\n",
        "        if predictions:\n",
        "            vote_counts = Counter(predictions)\n",
        "            most_common_count = vote_counts.most_common(1)[0][1]\n",
        "            # Only assign majority if it appears more than once\n",
        "            if most_common_count > 1:\n",
        "                majority_vote = vote_counts.most_common(1)[0][0]\n",
        "            else:\n",
        "                majority_vote = \"NOT DECIDED\"\n",
        "        else:\n",
        "            majority_vote = None\n",
        "        majority_votes.append(majority_vote)\n",
        "\n",
        "    # Add majority vote column\n",
        "    wide_df['majority_vote'] = majority_votes\n",
        "\n",
        "    wide_df.to_csv(output_file, index=False)\n",
        "    print(f\"Results with majority vote saved to {output_file}\")\n",
        "    return wide_df"
      ],
      "metadata": {
        "id": "xyToz5yEWsJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_agreement(csv_file):\n",
        "    \"\"\"Calculate agreement - auto-detects binary mode if answers are good/bad.\"\"\"\n",
        "    df = pd.read_csv(csv_file)\n",
        "    base_cols = ['intervention_id', 'cq_id', 'intervention', 'cq', 'ground_truth']\n",
        "    model_cols = [col for col in df.columns if col not in base_cols]\n",
        "\n",
        "    for col in model_cols:\n",
        "\n",
        "        # Check if this model's predictions are in binary format\n",
        "        col_values = df[col].dropna()\n",
        "        binary_values = col_values.isin(['good', 'bad'])\n",
        "        is_binary = binary_values.mean() >= 0.9\n",
        "\n",
        "        if is_binary:\n",
        "            # Convert ground truth to binary for comparison\n",
        "            mapping = {'Useful': 'good', 'Invalid': 'bad', 'Unhelpful': 'bad'}\n",
        "            ground_truth_binary = df['ground_truth'].map(mapping)\n",
        "            agreement = (df[col] == ground_truth_binary).mean()\n",
        "        else:\n",
        "            # Direct comparison for 3-way data\n",
        "            agreement = (df[col] == df['ground_truth']).mean()\n",
        "\n",
        "        print(f\"{col}: {agreement:.3f}\")"
      ],
      "metadata": {
        "id": "ogwyrcHHkOku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#post eval pipeline\n",
        "# 1) make a neat table by aggregating answers by cq_id\n",
        "# 2) add column representing majority vote\n",
        "# 3) calculate agreement by model and by majority vote"
      ],
      "metadata": {
        "id": "WEzZScGqinMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reformat_results(\"/content/results.csv\")\n",
        "add_majority_vote(\"/content/results_wide.csv\")\n",
        "calculate_agreement(\"/content/results_with_majority.csv\")"
      ],
      "metadata": {
        "id": "t3880Bp_XtUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNoW0wxVDgxIpadt7x3a98i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}